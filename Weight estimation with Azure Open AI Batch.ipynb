{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## From Excel to batch"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openpyxl\n",
        "%pip install openai azure-core dotenv"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: openpyxl in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.1.5)\nRequirement already satisfied: et-xmlfile in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openpyxl) (2.0.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: openai in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.93.0)\nRequirement already satisfied: azure-core in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.34.0)\nRequirement already satisfied: dotenv in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.9.9)\nRequirement already satisfied: anyio<5,>=3.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (0.10.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (2.11.7)\nRequirement already satisfied: sniffio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from openai) (4.14.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\nRequirement already satisfied: idna>=2.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\nRequirement already satisfied: certifi in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\nRequirement already satisfied: requests>=2.21.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-core) (2.32.4)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-core) (1.17.0)\nRequirement already satisfied: python-dotenv in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from dotenv) (1.1.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.21.0->azure-core) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.21.0->azure-core) (2.4.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "INPUT_EXCEL_FILE        = 'files/1_Items.xlsx'\n",
        "AZURE_ENDPOINT          = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
        "AZURE_OPENAI_API_KEY    = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "DEPLOYMENT_NAME         = os.getenv(\"DEPLOYMENT_NAME\")\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1751922139588
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def create_prompt_batch_jsonl(\n",
        "    excel_path,\n",
        "    output_jsonl_path,\n",
        "    sheet_name=\"Sheet2\",\n",
        "    row_limit=1,\n",
        "    material_number_col=\"Material Number\",\n",
        "    name_col=\"Material Name\",\n",
        "    category_col=\"Material Category Description\",\n",
        "    vendor_name_col=\"Vendor Common Name\",\n",
        "    country_col=\"Vendor Country\",\n",
        "    deployment_name=\"gpt-4o-2-batch\",\n",
        "    save_file=True\n",
        "):\n",
        "    excel_path = Path(excel_path)\n",
        "    output_jsonl_path = Path(output_jsonl_path)\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_excel(excel_path, sheet_name=sheet_name, engine='openpyxl')\n",
        "    df = df[[material_number_col, name_col, category_col, country_col, vendor_name_col]].fillna("").head(row_limit)\n",
        "\n",
        "    # Define schema \n",
        "    json_schema = {\n",
        "        \"name\": \"ItemEstimationResponse\",\n",
        "        \"strict\": True,\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"material_number\": {\"type\": \"string\"},\n",
        "                \"name\": {\"type\": \"string\"},\n",
        "                \"weight_g\": {\"type\": \"string\"},\n",
        "                \"material_composition\": {\"type\": \"string\"},\n",
        "                \"weight_estimation_explanation\": {\"type\": \"string\"},\n",
        "                \"confidence\": {\"type\": \"string\"}\n",
        "            },\n",
        "            \"required\": [\n",
        "                \"material_number\",\n",
        "                \"name\",\n",
        "                \"weight_g\",\n",
        "                \"material_composition\",\n",
        "                \"weight_estimation_explanation\",\n",
        "                \"confidence\"\n",
        "            ],\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "\n",
        "    jsonl_entries = []\n",
        "    for idx, row in df.iterrows():\n",
        "        user_prompt = (\n",
        "            f\"Estimate the weight in grams and simplified material composition for: Name:{row[name_col]}. \"\n",
        "            f\"Category: {row[category_col]}. Material_Number: {row[material_number_col]}. Country: {row[country_col]}. Vendor_Name: {row[vendor_name_col]}. \"\n",
        "            f\"Return the result as JSON with fields: material_number, name, weight_g, material_composition, weight_estimation_explanation, and confidence.\"\n",
        "            f\"Use the following format for each value: -\\\"name\\\":{row[name_col]}  -\\\"material_number\\\":{row[material_number_col]}.\"\n",
        "            f\"-\\\"weight_g\\\":\\\"300\\\"(Only integers, do not add gr, grams, or any other letter)\"\n",
        "            f\"-\\\"material_composition\\\":\\\"Brass body with chrome plating; internal rubber seals.\\\",\"\n",
        "            f\"-\\\"weight_estimation_explanation\\\":\\\"Standard faucet weight, including drain assembly.\\\",\"\n",
        "            f\"-\\\"confidence\\\":\\\"8\\\". Integer from 0 to 10, 10 being most confident\"\n",
        "        )\n",
        "        entry = {\n",
        "            \"custom_id\": f\"task-{idx}\",\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/chat/completions\",\n",
        "            \"temperature\": 0.0,\n",
        "            \"body\": {\n",
        "                \"model\": deployment_name,\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an AI assistant that helps people estimate item weights and material composition.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": user_prompt\n",
        "                    }\n",
        "                ],\n",
        "                \"response_format\": {\n",
        "                    \"type\": \"json_schema\",\n",
        "                    \"json_schema\": json_schema\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        jsonl_entries.append(entry)\n",
        "\n",
        "    if save_file:\n",
        "        output_jsonl_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with output_jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "            for entry in jsonl_entries:\n",
        "                f.write(json.dumps(entry) + '\\n')\n",
        "        print(f\"✅ JSONL file written to {output_jsonl_path}\")\n",
        "\n",
        "    return jsonl_entries\n",
        "\n",
        "\n",
        "# Create Batch input file\n",
        "entries = create_prompt_batch_jsonl(\n",
        "    excel_path=INPUT_EXCEL_FILE,\n",
        "    output_jsonl_path=\"files/2_input_batch_structured.jsonl\",\n",
        "    sheet_name=\"Sheet2\",\n",
        "    row_limit=50000,\n",
        "    material_number_col=\"Material Number\",\n",
        "    name_col=\"Material Name\",\n",
        "    category_col=\"Material Category Description\",\n",
        "    vendor_name_col=\"Vendor Common Name\",\n",
        "    country_col=\"Vendor Country\",\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    save_file=True\n",
        ")\n",
        "\n",
        "print(entries[0])  # Peek at the first one"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "✅ JSONL file written to files/2_input_batch_structured.jsonl\n{'custom_id': 'task-0', 'method': 'POST', 'url': '/chat/completions', 'temperature': 0.0, 'body': {'model': 'gpt-4o-2-batch', 'messages': [{'role': 'system', 'content': 'You are an AI assistant that helps people estimate item weights and material composition.'}, {'role': 'user', 'content': 'Estimate the weight in grams and simplified material composition for: Name:BLOCK, CLUSTER. Category: Plastics-Foam. Material_Number: 1385063. Country: United States of America. Vendor_Name: EFP LLC. Return the result as JSON with fields: material_number, name, weight_g, material_composition, weight_estimation_explanation, and confidence.Use the following format for each value: -\"name\":BLOCK, CLUSTER  -\"material_number\":1385063.-\"weight_g\":\"300\"(Only integers, do not add gr, grams, or any other letter)-\"material_composition\":\"Brass body with chrome plating; internal rubber seals.\",-\"weight_estimation_explanation\":\"Standard faucet weight, including drain assembly.\",-\"confidence\":\"8\". Integer from 0 to 10, 10 being most confident'}], 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'ItemEstimationResponse', 'strict': True, 'schema': {'type': 'object', 'properties': {'material_number': {'type': 'string'}, 'name': {'type': 'string'}, 'weight_g': {'type': 'string'}, 'material_composition': {'type': 'string'}, 'weight_estimation_explanation': {'type': 'string'}, 'confidence': {'type': 'string'}}, 'required': ['material_number', 'name', 'weight_g', 'material_composition', 'weight_estimation_explanation', 'confidence'], 'additionalProperties': False}}}}}\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1751922145505
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "def run_batch_job(\n",
        "    endpoint: str,\n",
        "    api_key: str,\n",
        "    deployment_name: str,\n",
        "    input_jsonl_path: str,\n",
        "    output_jsonl_path: str,\n",
        "    poll_interval: int = 10\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Uploads .jsonl to Azure OpenAI, starts & polls batch job,\n",
        "    then downloads the output. Returns path to results.\n",
        "    \"\"\"\n",
        "    print(api_key)\n",
        "    client = AzureOpenAI(\n",
        "        api_key=api_key,\n",
        "        api_version=\"2025-03-01-preview\",\n",
        "        azure_endpoint = endpoint\n",
        "    )\n",
        "\n",
        "    input_path = Path(input_jsonl_path)\n",
        "    if not input_path.exists():\n",
        "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
        "\n",
        "    # 1) Upload a file with a purpose of \"batch\"\n",
        "    file = client.files.create(\n",
        "        file=open(input_path, \"rb\"), \n",
        "        purpose=\"batch\",\n",
        "        extra_body={\"expires_after\":{\"seconds\": 1209600, \"anchor\": \"created_at\"}} # 14 days - Optional you can set to a number between 1209600-2592000. This is equivalent to 14-30 days\n",
        "    )\n",
        "    file_id = file.id\n",
        "    print(f\"Uploaded file ID: {file_id}\")\n",
        "    print(file.model_dump_json(indent=2))\n",
        "    \n",
        "    # Wait until the uploaded file is in processed state\n",
        "    import time\n",
        "    import datetime \n",
        "\n",
        "    status = \"pending\"\n",
        "    while status != \"processed\":\n",
        "        time.sleep(15)\n",
        "        file_response = client.files.retrieve(file_id)\n",
        "        status = file_response.status\n",
        "        print(f\"{datetime.datetime.now()} File Id: {file_id}, Status: {status}\")\n",
        "\n",
        "    # 2) Create and submit a batch job with the file\n",
        "\n",
        "    batch_response = client.batches.create(\n",
        "        input_file_id=file_id,\n",
        "        endpoint=\"/chat/completions\",\n",
        "        completion_window=\"24h\",\n",
        "    )\n",
        "\n",
        "    # Save batch ID for later use\n",
        "    batch_id = batch_response.id\n",
        "\n",
        "    print(batch_response.model_dump_json(indent=2))\n",
        "    print(f\"Batch job created: {batch_id} | Status: {batch_response.status}\")\n",
        "\n",
        "    # 3) Poll and track job progress\n",
        "    import time\n",
        "    import datetime \n",
        "\n",
        "    status = \"validating\"\n",
        "    while status not in (\"completed\", \"failed\", \"canceled\"):\n",
        "        time.sleep(poll_interval)\n",
        "        batch_response = client.batches.retrieve(batch_id)\n",
        "        status = batch_response.status\n",
        "        print(f\"{datetime.datetime.now()} Batch Id: {batch_id},  Status: {status}\")\n",
        "\n",
        "    # 4) Download\n",
        "    output_path = Path(output_jsonl_path)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    # client.download_file(batch_id, str(output_path))\n",
        "\n",
        "    import json\n",
        "\n",
        "    output_file_id = batch_response.output_file_id\n",
        "\n",
        "    if not output_file_id:\n",
        "        output_file_id = batch_response.error_file_id\n",
        "\n",
        "    if output_file_id:\n",
        "        file_response = client.files.content(output_file_id)\n",
        "        raw_responses = file_response.text.strip().split('\\n')\n",
        "\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "            for raw_response in raw_responses:\n",
        "                json_response = json.loads(raw_response)\n",
        "                json_line = json.dumps(json_response)\n",
        "                outfile.write(json_line + '\\n')\n",
        "\n",
        "    return output_path\n",
        "    print(f\"✅ Results saved to {output_path}\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1751922145864
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_JSONL_FILE  = \"files/2_input_batch_structured.jsonl\"\n",
        "OUTPUT_JSONL_FILE = \"files/3_output_batch_structured.jsonl\"\n",
        "POLL_INTERVAL_SEC = 300\n",
        "\n",
        "# Kick off job\n",
        "result_path = run_batch_job(\n",
        "    endpoint=AZURE_ENDPOINT,\n",
        "    api_key=AZURE_OPENAI_API_KEY,\n",
        "    deployment_name=DEPLOYMENT_NAME,\n",
        "    input_jsonl_path=INPUT_JSONL_FILE,\n",
        "    output_jsonl_path=OUTPUT_JSONL_FILE,\n",
        "    poll_interval=POLL_INTERVAL_SEC\n",
        ")\n",
        "\n",
        "# Peek first 5 lines of the output file\n",
        "print(result_path.read_text().splitlines()[:5])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "53a51a01c5564f1eb88515168c030bcb\nUploaded file ID: file-c0180149f3a94c76a11f9f4d644aaa61\n{\n  \"id\": \"file-c0180149f3a94c76a11f9f4d644aaa61\",\n  \"bytes\": 70937492,\n  \"created_at\": 1751922147,\n  \"filename\": \"2_input_batch_structured.jsonl\",\n  \"object\": \"file\",\n  \"purpose\": \"batch\",\n  \"status\": \"processed\",\n  \"expires_at\": 1753131747,\n  \"status_details\": null\n}\n2025-07-07 21:02:42.897633 File Id: file-c0180149f3a94c76a11f9f4d644aaa61, Status: processed\n{\n  \"id\": \"batch_254c14dd-de87-4625-ba7c-2fdc870d0e2f\",\n  \"completion_window\": \"24h\",\n  \"created_at\": 1751922163,\n  \"endpoint\": \"/chat/completions\",\n  \"input_file_id\": \"file-c0180149f3a94c76a11f9f4d644aaa61\",\n  \"object\": \"batch\",\n  \"status\": \"validating\",\n  \"cancelled_at\": null,\n  \"cancelling_at\": null,\n  \"completed_at\": null,\n  \"error_file_id\": \"\",\n  \"errors\": null,\n  \"expired_at\": null,\n  \"expires_at\": 1752008563,\n  \"failed_at\": null,\n  \"finalizing_at\": null,\n  \"in_progress_at\": null,\n  \"metadata\": null,\n  \"output_file_id\": \"\",\n  \"request_counts\": {\n    \"completed\": 0,\n    \"failed\": 0,\n    \"total\": 0\n  }\n}\nBatch job created: batch_254c14dd-de87-4625-ba7c-2fdc870d0e2f | Status: validating\n2025-07-07 21:07:43.362870 Batch Id: batch_254c14dd-de87-4625-ba7c-2fdc870d0e2f,  Status: in_progress\n2025-07-07 21:12:43.544039 Batch Id: batch_254c14dd-de87-4625-ba7c-2fdc870d0e2f,  Status: in_progress\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1751922119535
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Azure Batch to final excel file.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_jsonl_to_excel(jsonl_path, excel_path, csv_path, save_files=True):\n",
        "    jsonl_path = Path(jsonl_path)\n",
        "    excel_path = Path(excel_path)\n",
        "    csv_path = Path(csv_path)\n",
        "    \n",
        "    jsonl_data = []\n",
        "    with jsonl_path.open(\"r\", encoding=\"utf-8\") as file:\n",
        "        for i, line in enumerate(tqdm(file, desc=\"Reading JSONL\"), 1):\n",
        "            try:\n",
        "                jsonl_data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Skipping line {i}: JSON decode error - {e}\")\n",
        "\n",
        "    records = []\n",
        "    for entry in jsonl_data:\n",
        "        body = entry.get(\"response\", {}).get(\"body\", {})\n",
        "        choices = body.get(\"choices\", [])\n",
        "        usage = body.get(\"usage\", {})\n",
        "        #content_str = choices[0][\"message\"][\"content\"] if choices else \"{}\"\n",
        "        if choices and isinstance(choices[0], dict):\n",
        "            message = choices[0].get(\"message\", {})\n",
        "            content_str = message.get(\"content\", \"{}\")\n",
        "        else:\n",
        "            content_str = \"{}\"\n",
        "\n",
        "        try:\n",
        "            content = json.loads(content_str)\n",
        "        except json.JSONDecodeError:\n",
        "            content = {}\n",
        "\n",
        "        record = {\n",
        "            \"Material Number\": content.get(\"material_number\", \"\"),\n",
        "            \"Name\": content.get(\"name\", \"\"),\n",
        "            \"weight_g\": content.get(\"weight_g\", \"\"),\n",
        "            \"material_composition\": content.get(\"material_composition\", \"\"),\n",
        "            \"weight_estimation_explanation\": content.get(\"weight_estimation_explanation\", \"\"),\n",
        "            \"confidence\": content.get(\"confidence\", \"\"),\n",
        "            \"completion_tokens\": usage.get(\"completion_tokens\", \"\"),\n",
        "            \"prompt_tokens\": usage.get(\"prompt_tokens\", \"\"),\n",
        "            \"total_tokens\": usage.get(\"total_tokens\", \"\")\n",
        "        }\n",
        "        records.append(record)\n",
        "        print(record)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    if save_files:\n",
        "        df.to_excel(excel_path, index=False)\n",
        "        print(f\"Excel written to {excel_path}\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"CSV written to {csv_path}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Example usage\n",
        "df = extract_jsonl_to_excel(\n",
        "    \"files/3_output_batch_structured.jsonl\",\n",
        "    \"files/4_extracted_data.xlsx\",\n",
        "    \"files/4_extracted_data.csv\",\n",
        "    save_files=True  # Will overwrite any existing files\n",
        ")\n",
        "\n",
        "df.head(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1751922120372
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
